{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import math\n",
    "load_dotenv()\n",
    "base_path = Path.home() / Path(os.environ.get(\"DATA_PATH\"))\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the behavior tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "   base_path/ Path(\"behaviors.tsv\"), \n",
    "    sep=\"\\t\",\n",
    "    names=[\"impressionId\",\"userId\",\"timestamp\",\"click_history\",\"impressions\"])\n",
    "\n",
    "print(f\"The dataset originally consist of {len(df)} number of interactions.\")\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\n",
    "    base_path/ Path(\"news.tsv\"), \n",
    "    sep=\"\\t\",\n",
    "    names=[\"itemId\",\"category\",\"subcategory\",\"title\",\"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n",
    "print(f\"The article data consist in total of {len(news)} number of articles.\")\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = news['category'].unique()\n",
    "news[\"num_category\"] = news[\"category\"].factorize()[0]\n",
    "article_category = news.set_index(\"itemId\")[\"num_category\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['impressions'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the impressions in two columns clicked items and presented slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_impression(impression_list):\n",
    "    list_of_strings = impression_list.split()\n",
    "    click = [x.split('-')[0] for x in list_of_strings if x.split('-')[1] == '1']\n",
    "    non_click = [x.split('-')[0] for x in list_of_strings]\n",
    "    return click,non_click\n",
    "\n",
    "\n",
    "df['click'], df['presented_slate'] = zip(*df['impressions'].map(process_impression))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = pd.read_csv(\n",
    "#     base_path/ Path(\"news.tsv\"), \n",
    "#     sep=\"\\t\",\n",
    "#     names=[\"itemId\",\"category\",\"subcategory\",\"title\",\"abstract\",\"url\",\"title_entities\",\"abstract_entities\"])\n",
    "# print(f\"The article data consist in total of {len(news)} number of articles.\")\n",
    "# news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the valid article ids from the news.tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_article_ids = set(news['itemId'])\n",
    "print(len(news['itemId']))\n",
    "def filter_click_list(click_list):\n",
    "    return [item for item in click_list if item in valid_article_ids]\n",
    "def filter_click_history_list(click_list):\n",
    "    if pd.notna(click_list['click_history']):\n",
    "     return [item for item in click_list['click_history'].split() if item in valid_article_ids]\n",
    "    else:\n",
    "     return []\n",
    "behaviors_df=df\n",
    "\n",
    "behaviors_df['click'] = behaviors_df['click'].apply(filter_click_list)\n",
    "behaviors_df['presented_slate'] = behaviors_df['presented_slate'].apply(filter_click_list)\n",
    "behaviors_df['click_history'] = behaviors_df.apply(filter_click_history_list,axis=1)\n",
    "# behaviors_df[behaviors_df['click'].apply(lambda x: any(item in valid_article_ids for item in x))]\n",
    "# filtered_behaviors_df = filtered_behaviors_df[filtered_behaviors_df['presented_slate'].apply(lambda x: any(item in valid_article_ids for item in x))]\n",
    "filtered_behaviors_df = behaviors_df[behaviors_df['click'].apply(lambda x: len(x) > 0)]\n",
    "filtered_behaviors_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[news['itemId'] == 'N82719']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe for the user choice model. Only keep impressions with a single click as they dont induce a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_choice_data = filtered_behaviors_df[\n",
    "    (df['click'].apply(lambda x: len(x) == 1 or (len(x) == 0 and isinstance(x, list))))\n",
    "].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_choice_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep this for the RL algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2 = filtered_behaviors_df[\n",
    "    (df['click'].apply(lambda x: len(x) != 1 or (len(x) == 0 and isinstance(x, list))))\n",
    "].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Interaction contains impression of size between 5 and 10 while interaction_all between 5 and greater. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = filtered_df_2.sample(n=100000, random_state=42)\n",
    "filtered_df_2 = filtered_df_2.drop(test_data.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the entropy_based_diversity to calculate the diversity of user in the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def click_history_diversity(row) :\n",
    "        items_hist = row[\"click_history\"]\n",
    "        categories = [\n",
    "            article_category.get(article_id, 0) for article_id in items_hist\n",
    "        ]\n",
    "        category_counts = [categories.count(i) for i in range(0, 18)]\n",
    "        total_count = sum(category_counts)\n",
    "        probs = [count / total_count for count in category_counts]\n",
    "        entropy = 0\n",
    "        for prob in probs:\n",
    "            if prob > 0:\n",
    "                entropy-= prob * math.log2(prob)\n",
    "        # score = sum(1 for x in count_categories if x > 0) / 18\n",
    "        return entropy\n",
    "\n",
    "def entropy_based_diversity(row):\n",
    "\n",
    "  # Normalize the counts to get probabilities\n",
    "    items_hist = row[\"click_history\"]\n",
    "    categories = [\n",
    "        article_category.get(article_id, 0) for article_id in items_hist\n",
    "    ]\n",
    "    category_counts = [categories.count(i) for i in range(0, 18)]\n",
    "    probs = category_counts / np.sum(category_counts)\n",
    "\n",
    "    # Handle zero probabilities (avoid log of zero)\n",
    "    probs = np.where(probs > 0, probs, 1e-10)\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(probs * np.log2(probs))\n",
    "\n",
    "    # Normalize entropy (optional, comment out if not needed)\n",
    "    diversity_score = entropy / np.log2(len(category_counts))\n",
    "\n",
    "    return diversity_score\n",
    "test_data['diversity_score']=test_data.apply(entropy_based_diversity,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['diversity_score'].hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['diversity_score'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cold start users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_scores = test_data[test_data['diversity_score'] == 1.3542876093347509e-08]\n",
    "zero_scores.reset_index(inplace=True, drop=True)\n",
    "# feather_path_test=base_path/ Path(\"MINDlarge_train/entropy_coldstart_test_50.feather\")\n",
    "# zero_scores.to_feather(feather_path_test)\n",
    "zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_scores = test_data[test_data['diversity_score'] > 1.3542876093347509e-07]\n",
    "non_zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "non_zero_scores['diversity_score'].hist(bins=60)\n",
    "plt.xlabel('Categorical entropy')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = non_zero_scores['diversity_score'].quantile(0.25)\n",
    "q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specialist users in the first quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists=non_zero_scores[non_zero_scores['diversity_score'] < q1]\n",
    "specialists.reset_index(inplace=True, drop=True)\n",
    "# feather_path_test=base_path/ Path(\"MINDlarge_train/entropy_specialist_test_50.feather\")\n",
    "# specialists.to_feather(feather_path_test)\n",
    "specialists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalist users in the second quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalists=non_zero_scores[non_zero_scores['diversity_score'] > q1]\n",
    "generalists.reset_index(inplace=True, drop=True)\n",
    "# feather_path_test=base_path/ Path(\"MINDlarge_train/entropy_generalist_test_50.feather\")\n",
    "# generalists.to_feather(feather_path_test)\n",
    "generalists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_feather(\n",
    "            base_path / Path(\"MINDlarge_train/news_glove_embed_50.feather\")\n",
    "        )\n",
    "embedding_dict = dict(zip(news_df[\"itemId\"], news_df[\"embedding\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Diversity Dissimilarity training data\n",
    "# import torch\n",
    "# from torch._tensor import Tensor\n",
    "# def diversity_dissimilarity(items_hist):\n",
    "#     item_list_hist = [\n",
    "#             embedding_dict.get(key, [])\n",
    "#             for key in items_hist\n",
    "#             if embedding_dict.get(key, []) is not None\n",
    "#             and len(embedding_dict.get(key, [])) > 0\n",
    "#         ]\n",
    "#     item_tensor = [\n",
    "#         torch.tensor(array, dtype=torch.float) for array in item_list_hist\n",
    "#     ]\n",
    "#     if len(item_tensor) >= 2:\n",
    "#         tensors = item_tensor\n",
    "#         n = len(tensors)\n",
    "#         similarity_matrix = torch.zeros(n, n)\n",
    "\n",
    "#         # Compute similarity matrix\n",
    "#         for i in range(n):\n",
    "#             for j in range(i + 1, n):\n",
    "#                 similarity_matrix[i, j] = torch.dot(tensors[i], tensors[j]) / (\n",
    "#                     torch.norm(tensors[i]) * torch.norm(tensors[j])\n",
    "#                 )\n",
    "#                 similarity_matrix[j, i] = similarity_matrix[\n",
    "#                     i, j\n",
    "#                 ]  # Similarity matrix is symmetric\n",
    "\n",
    "#         # Calculate diversity\n",
    "#         total_diversity = (\n",
    "#             torch.sum(1 - similarity_matrix) - n\n",
    "#         )  # Exclude diagonal elements\n",
    "#         diversity_measure = total_diversity / ((n / 2) * (n - 1))\n",
    "#     else:\n",
    "#         diversity_measure = torch.tensor(0.0)\n",
    "\n",
    "#     return diversity_measure\n",
    "\n",
    "filtered_df_2['diversity_score'] = filtered_df_2.apply(entropy_based_diversity,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_3=filtered_df_2\n",
    "filtered_df_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filtered_df_2_3['diversity_score'] = filtered_df_2_3['diversity_score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_path=base_path/ Path(\"MINDlarge_train/div_entropy_50.feather\")\n",
    "filtered_df_2_3.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_div=pd.read_feather(feather_path)\n",
    "data_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.kdeplot(data_div['diversity_score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5  # Replace with your desired threshold value\n",
    "count_above_threshold = (data_div['diversity_score'] > threshold).sum()\n",
    "\n",
    "print(f\"Number of rows with diversity_score greater than {threshold}:\", count_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_div['diversity_score'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_div1 = data_div[data_div['diversity_score'] > 1.3542876093347509e-07]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_div1['diversity_score'].hist(bins=60)\n",
    "plt.xlabel('Categorical entropy')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'data_div' is your DataFrame\n",
    "scaler = MinMaxScaler(feature_range=(0, 2.2))  # Scales between 0 and 1\n",
    "\n",
    "# Fit the scaler to the 'diversity_score' column\n",
    "# scaler = StandardScaler()\n",
    "scaler.fit(data_div[['diversity_score']])\n",
    "\n",
    "\n",
    "# Transform the data and store it in a new column named 'scaled_diversity'\n",
    "data_div['scaled_diversity'] = scaler.transform(data_div[['diversity_score']])\n",
    "data_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scaling(value, threshold=0.25, new_max=1):\n",
    "  \"\"\"Scales a value to be closer to new_max if it's above threshold.\n",
    "\n",
    "  Args:\n",
    "      value: The value to be scaled.\n",
    "      threshold: The threshold value above which scaling is applied.\n",
    "      new_max: The new maximum value for scaled data (defaults to 1).\n",
    "\n",
    "  Returns:\n",
    "      The scaled value.\n",
    "  \"\"\"\n",
    "\n",
    "  if value <= threshold:\n",
    "    return value  # No change for values below the threshold\n",
    "  else:\n",
    "    # Linear scaling to compress values between threshold and new_max\n",
    "    slope = (new_max - threshold) / (1 - threshold)\n",
    "    scaled_value = slope * (value - threshold) + threshold\n",
    "    return min(scaled_value, new_max)  # Ensures no value exceeds new_max\n",
    "\n",
    "# Assuming 'data_div' is your DataFrame\n",
    "data_div['scaled_diversity'] = data_div['diversity_score'].apply(custom_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_div['scaled_diversity'].max())\n",
    "data_div['scaled_diversity'].hist(bins=60)\n",
    "plt.xlabel('Values in column')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Diversity score based on user history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1=filtered_df_2.explode('click', ignore_index=True)\n",
    "filtered_df_2_1=filtered_df_2_1.drop(columns=['index'])\n",
    "filtered_df_2_1\n",
    "# feather_path=base_path/ Path(\"MINDlarge_train/interaction_all.feather\")\n",
    "# filtered_df_2_1.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_scores=zero_scores.explode('click', ignore_index=True)\n",
    "zero_scores=zero_scores.drop(columns=['index'])\n",
    "zero_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalists=generalists.explode('click', ignore_index=True)\n",
    "generalists=generalists.drop(columns=['index'])\n",
    "generalists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists=specialists.explode('click', ignore_index=True)\n",
    "specialists=specialists.drop(columns=['index'])\n",
    "specialists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news['num_category'] = news['category'].factorize()[0]\n",
    "# article_category_map = news.set_index('itemId')['num_category'].to_dict()\n",
    "\n",
    "# def get_clicked_category_list(row):\n",
    "#     # if pd.notna(row['click_history']):\n",
    "#         # Use a list comprehension to create the list of categories\n",
    "#     categories = [article_category_map.get(article_id, 0) for article_id in row['click']]\n",
    "#     count_categories = [categories.count(i) for i in range(0, 18)]\n",
    "#     # else:\n",
    "#     #     # If 'click_history' is NaN, assign a list of zeros\n",
    "#     #     count_categories = []\n",
    "#     return count_categories\n",
    "# def get_clicked_hist_category_list(row):\n",
    "#     # if pd.notna(row['click_history']):\n",
    "#         # Use a list comprehension to create the list of categories\n",
    "#     categories = [article_category_map.get(article_id, 0) for article_id in row['click_history']]\n",
    "#     count_categories = [categories.count(i) for i in range(0, 18)]\n",
    "#     # else:\n",
    "#     #     # If 'click_history' is NaN, assign a list of zeros\n",
    "#     #     count_categories = []\n",
    "#     return count_categories\n",
    "\n",
    "# # Apply the function to create the new column\n",
    "# filtered_df_2['clicked_category_list'] = filtered_df_2.apply(get_clicked_category_list, axis=1)\n",
    "# filtered_df_2['clicked_hist_category_list'] = filtered_df_2.apply(get_clicked_hist_category_list, axis=1)\n",
    "# filtered_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correlation_matrix = filtered_df_2['clicked_category_list'].apply(pd.Series).corrwith(filtered_df_2['clicked_hist_category_list'].apply(pd.Series))\n",
    "# correlation_matrix = filtered_df_2.apply(lambda x: pd.Series(x['clicked_category_list']).corr(pd.Series(x['clicked_hist_category_list'])), axis=1)\n",
    "# # Print the correlation matrix\n",
    "# print(\"Correlation Matrix:\")\n",
    "# print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# sns.heatmap([correlation_matrix], annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.2)\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[news['itemId'] == 'N25587']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_filtered_df_2 = filtered_df_2_1.sort_values(by='timestamp').groupby('userId').agg({\n",
    "#     'impressions': lambda x: list(x),\n",
    "#     'timestamp': lambda x: list(x),\n",
    "#     'click_history': lambda x: list(x),\n",
    "#     'click': lambda x: list(x),\n",
    "#     'presented_slate': lambda x: list(x),   \n",
    "# }).reset_index()\n",
    "# grouped_filtered_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = grouped_filtered_df_2['click'].apply(len).max()\n",
    "# mean_length = grouped_filtered_df_2['click'].apply(len).mean()\n",
    "# min_length = grouped_filtered_df_2['click'].apply(len).min()\n",
    "# mode_length = grouped_filtered_df_2['click'].apply(len).mode().iloc[0]\n",
    "# median_length = grouped_filtered_df_2['click'].apply(len).median()\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Max length:\", max_length)\n",
    "# print(\"Mean length:\", mean_length)\n",
    "# print(\"Min length:\", min_length)\n",
    "# print(\"Mode length:\", mode_length)\n",
    "# print(\"Median length:\", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_filtered_df_2_t = filtered_df_2_1.sort_values(by='timestamp').groupby('timestamp').agg({\n",
    "#     'impressions': lambda x: list(x),\n",
    "#     'click_history': lambda x: list(x),\n",
    "#     'click': lambda x: list(x),\n",
    "#     'presented_slate': lambda x: list(x),   \n",
    "# }).reset_index()\n",
    "# grouped_filtered_df_2_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = grouped_filtered_df_2_t['timestamp'].apply(len).max()\n",
    "# mean_length = grouped_filtered_df_2_t['timestamp'].apply(len).mean()\n",
    "# min_length = grouped_filtered_df_2_t['timestamp'].apply(len).min()\n",
    "# mode_length = grouped_filtered_df_2_t['timestamp'].apply(len).mode().iloc[0]\n",
    "# median_length = grouped_filtered_df_2_t['timestamp'].apply(len).median()\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Max length:\", max_length)\n",
    "# print(\"Mean length:\", mean_length)\n",
    "# print(\"Min length:\", min_length)\n",
    "# print(\"Mode length:\", mode_length)\n",
    "# print(\"Median length:\", median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = news['category'].unique()\n",
    "news[\"num_category\"] = news[\"category\"].factorize()[0]\n",
    "article_category = news.set_index(\"itemId\")[\"num_category\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_num_mapping = {category: num for num, category in enumerate(news['category'].unique())}\n",
    "\n",
    "# # Create a new column 'num' in the news DataFrame based on the mapping\n",
    "# news['num_category'] = news['category'].map(category_num_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_num_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in filtered_df_2_1[0]:\n",
    "#     print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news['num_category'] = news['category'].factorize()[0]\n",
    "# article_category_map = news.set_index('itemId')['num_category'].to_dict()\n",
    "# # def get_category_list(row):\n",
    "# #     if pd.notna(row['click_history']):\n",
    "# #         # Use a list comprehension to create the list of categories\n",
    "# #         categories=[0] * 18    \n",
    "# #         for article_id in row['click_history'].split():\n",
    "            \n",
    "# #             i=article_category_map.get(article_id, -1)\n",
    "# #             if i!=-1:\n",
    "# #                 categories[i]+=1\n",
    "# #     else:\n",
    "# #         # If 'click_history' is NaN, assign an empty list\n",
    "# #         categories = []\n",
    "# #     return categories\n",
    "# def get_category_list(row):\n",
    "#     if pd.notna(row['click_history']):\n",
    "#         # Use a list comprehension to create the list of categories\n",
    "#         categories = [article_category_map.get(article_id, 0) for article_id in row['click_history'].split()]\n",
    "#         count_categories = [categories.count(i) for i in range(0, 18)]\n",
    "#     else:\n",
    "#         # If 'click_history' is NaN, assign a list of zeros\n",
    "#         count_categories = []\n",
    "#     return count_categories\n",
    "\n",
    "# # Apply the function to create the new column\n",
    "# filtered_df_2_1['category_list'] = filtered_df_2_1.apply(get_category_list, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_category_map.get('N45706', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feather_path=base_path/ Path(\"MINDlarge_train/category.feather\")\n",
    "# filtered_df_2_1.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(article_category_map.get('N88753', 0))\n",
    "# article_category_map\n",
    "# for article_id in filtered_df_2_1['click_history']:\n",
    "#     print(article_id)\n",
    "#     print(type(article_id))\n",
    "#     print(article_category_map.get(article_id, 0))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# def sampled_user(count_list):\n",
    "#     probabilities_tensor = torch.tensor(count_list, dtype=torch.float32)\n",
    "#     probabilities_normalized = probabilities_tensor / probabilities_tensor.sum()\n",
    "\n",
    "#     # Sample a value based on the probabilities\n",
    "#     sampled_value = torch.multinomial(probabilities_normalized, 1).item()\n",
    "\n",
    "#     # Create the sampled tensor\n",
    "#     sampled_list_tensor = torch.zeros_like(probabilities_tensor, dtype=torch.float32)\n",
    "#     sampled_list_tensor[sampled_value] = 1\n",
    "#     return sampled_list_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_user(filtered_df_2_1['category_list'].loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_state=torch.Tensor(filtered_df_2_1['category_list'].loc[0])\n",
    "# user_state.shape\n",
    "# print(user_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items=filtered_df_2_1['presented_slate'].loc[0]\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# item_list = [article_category_map.get(key, []) for key in items]\n",
    "\n",
    "# remaining_items = 30 - len(item_list)\n",
    "# additional_values = random.choices(list(set(range(18)) - set(item_list)), k=remaining_items)\n",
    "\n",
    "# item_list.extend(additional_values)\n",
    "\n",
    "# random.shuffle(item_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(range(18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_of_list = len(item_list)\n",
    "\n",
    "# # Create a list of tensors\n",
    "# tensor_list = [torch.eye(19)[value] for value in item_list]\n",
    "\n",
    "# # Print the result\n",
    "# for i, tensor in enumerate(tensor_list):\n",
    "#     print(f\"Tensor for value {item_list[i]}:\\n{tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = filtered_df_2_1['userId'].loc[5]\n",
    "timestamp = filtered_df_2_1['timestamp'].loc[5]\n",
    "clicked_items = filtered_df_2_1[(filtered_df_2_1['userId'] == user_id) & (filtered_df_2_1['timestamp'] == timestamp)]['click'].tolist()\n",
    "print(clicked_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# numpy_array = np.copy(\n",
    "#            filtered_df_2_1[\"category_list\"].loc[20176]\n",
    "#         )\n",
    "# user_state = torch.Tensor(numpy_array)\n",
    "# print(user_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1.loc[35683]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the glove embedding as done in in a saved feather file in article_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_feather(\n",
    "            base_path / Path(\"MINDlarge_train/news_glove_embed_50.feather\")\n",
    "        )\n",
    "embedding_dict = dict(zip(news_df[\"itemId\"], news_df[\"embedding\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embedding(row):\n",
    "  \n",
    "    item_embeddings = [embedding_dict[item_id] for item_id in row['click_history'] if item_id in embedding_dict]\n",
    "    item_embeddings = [embedding for embedding in item_embeddings if embedding is not None]\n",
    "    if item_embeddings:\n",
    "        user_embedding = sum(item_embeddings) / len(item_embeddings)\n",
    "    else:\n",
    "\n",
    "        user_embedding = None\n",
    "    return user_embedding\n",
    "\n",
    "# Apply the function to create the new column\n",
    "# filtered_df_2_1['observed_state'] = filtered_df_2_1.apply(user_embedding, axis=1)\n",
    "zero_scores['observed_state'] = zero_scores.apply(user_embedding, axis=1)\n",
    "generalists['observed_state'] = generalists.apply(user_embedding, axis=1)\n",
    "specialists['observed_state'] = specialists.apply(user_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_empty_list = any(filtered_df_2_1['presented_slate'].apply(lambda x: len(x) < 2))\n",
    "\n",
    "if has_empty_list:\n",
    "    print(\"The 'presented_slate' column contains at least one empty list.\")\n",
    "else:\n",
    "    print(\"No empty lists found in the 'presented_slate' column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feather_path=base_path/ Path(\"MINDlarge_train/interaction_all_50.feather\")\n",
    "# filtered_df_2_1.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_rows = len(filtered_df_2_1)\n",
    "random_index = np.random.randint(0, num_rows)\n",
    "items = filtered_df_2_1[\"presented_slate\"].loc[random_index]\n",
    "\n",
    "item_list = [embedding_dict.get(key, []) for key in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=100\n",
    "all_vectors = [np.array(vector) for vector in embedding_dict.values()]\n",
    "\n",
    "    # Convert item_list vectors to NumPy arrays\n",
    "item_list_arrays = [np.array(vector) for vector in item_list]\n",
    "\n",
    "# Filter out vectors that are already in item_list\n",
    "available_vectors = [vector for vector in all_vectors if not any(np.array_equal(vector, item) for item in item_list_arrays)]\n",
    "\n",
    "# If there are less than k available vectors, you can decide how to handle this situation\n",
    "if len(available_vectors) < k:\n",
    "    raise ValueError(\"Not enough available vectors to select k random vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'click_history'\n",
    "empty_values_mask = filtered_df_2_1[column_name].apply(lambda x: isinstance(x, list) and len(x) == 0)\n",
    "\n",
    "\n",
    "# Display rows with empty or None values in the specified column\n",
    "rows_with_empty_values = filtered_df_2_1[empty_values_mask]\n",
    "print(rows_with_empty_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_1 = {key: value for key, value in embedding_dict.items() if value is not None}\n",
    "has_none_values = any(value is None for value in embedding_dict_1.values())\n",
    "print(\"Dictionary has None values:\", has_none_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_article_ids = set(embedding_dict_1.keys())\n",
    "\n",
    "def filter_click_list(click_list):\n",
    "    return [item for item in click_list if item in valid_article_ids]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filtered_df_2_1['presented_slate'] = filtered_df_2_1['presented_slate'].apply(filter_click_list)\n",
    "# filtered_df_2_1['click_history'] = filtered_df_2_1['click_history'].apply(filter_click_list)\n",
    "zero_scores['presented_slate'] = zero_scores['presented_slate'].apply(filter_click_list)\n",
    "zero_scores['click_history'] = zero_scores['click_history'].apply(filter_click_list)\n",
    "generalists['presented_slate'] = generalists['presented_slate'].apply(filter_click_list)\n",
    "generalists['click_history'] = generalists['click_history'].apply(filter_click_list)\n",
    "specialists['presented_slate'] = specialists['presented_slate'].apply(filter_click_list)\n",
    "specialists['click_history'] = specialists['click_history'].apply(filter_click_list)\n",
    "\n",
    "# filtered_df_2_1[filtered_df_2_1['click'].apply(lambda x: any(item in valid_article_ids for item in x))]\n",
    "# filtered_filtered_df_2_1 = filtered_filtered_df_2_1[filtered_filtered_df_2_1['presented_slate'].apply(lambda x: any(item in valid_article_ids for item in x))]\n",
    "# filtered_filtered_df_2_1 = filtered_df_2_1[filtered_df_2_1['click'].apply(lambda x: len(x) > 0)]\n",
    "# filtered_df_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_2_1['click'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df_2_2 = filtered_df_2_1[filtered_df_2_1['click'].isin(valid_article_ids)]\n",
    "zero_scores1 = zero_scores[zero_scores['click'].isin(valid_article_ids)]\n",
    "generalists1 = generalists[generalists['click'].isin(valid_article_ids)]\n",
    "specialists1 = specialists[specialists['click'].isin(valid_article_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_scores1.reset_index(inplace=True, drop=True)\n",
    "generalists1.reset_index(inplace=True, drop=True)\n",
    "specialists1.reset_index(inplace=True, drop=True)\n",
    "# filtered_df_2_2.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_path_test=base_path/ Path(\"MINDlarge_train/entropy_generalist_test_50.feather\")\n",
    "generalists1.to_feather(feather_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_path_test=base_path/ Path(\"MINDlarge_train/entropy_specialist_test_50.feather\")\n",
    "specialists1.to_feather(feather_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feather_path_test=base_path/ Path(\"MINDlarge_train/entropy_coldstart_test_50.feather\")\n",
    "zero_scores1.to_feather(feather_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data1.loc[30862]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalist_test = test_data1[test_data1['diversity_score'] > 0.4]\n",
    "\n",
    "# # Filter rows where 'diversity_score' is less than or equal to 0.4\n",
    "# specialist_test = test_data1[test_data1['diversity_score'] <= 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalists.reset_index(inplace=True, drop=True)\n",
    "specialists.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# feather_path=base_path/ Path(\"MINDlarge_train/interaction_all_50.feather\")\n",
    "# filtered_df_2_2.to_feather(feather_path)\n",
    "# feather_path_test=base_path/ Path(\"MINDlarge_train/generalist_test_50.feather\")\n",
    "# generalist_test.to_feather(feather_path_test)\n",
    "# feather_path_test1=base_path/ Path(\"MINDlarge_train/specialist_test_50.feather\")\n",
    "# specialist_test.to_feather(feather_path_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feather_path=base_path/ Path(\"MINDlarge_train/interaction_all_50.feather\")\n",
    "# filtered_df_2_2.to_feather(feather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=filtered_df_choice_data.loc[:, [\"userId\", \"click_history\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.drop_duplicates(subset=[\"userId\"])\n",
    "training_data = training_data.reset_index(drop=True)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# all_items=set(embedding_dict_1.keys())\n",
    "# # Reset the index\n",
    "\n",
    "# def get_negative_items(click_history):\n",
    "#     click_set = set(click_history)\n",
    "#     return np.random.choice(list(all_items - click_set), size=10, replace=False)\n",
    "\n",
    "# # Apply the function to the click_history column\n",
    "# training_data['negative_items'] = training_data['click_history'].apply(get_negative_items)\n",
    "\n",
    "\n",
    "\n",
    "# Drop the original click_history column\n",
    "# training_data = training_data.drop(columns=[\"click_history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_click_history(click_history):\n",
    "    if len(click_history) > 0:\n",
    "        user_embedding = click_history[0]\n",
    "        remaining_clicks = click_history[1:]\n",
    "        return user_embedding, remaining_clicks\n",
    "    else:\n",
    "        return None, []\n",
    "\n",
    "# Apply the function to the click_history column\n",
    "training_data[[\"user_embedding\", \"click_history\"]] = training_data[\"click_history\"].apply(split_click_history).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.explode('click_history', ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['click'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample=training_data.sample(n=80000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(set(u for u in training_sample['userId']))\n",
    "num_items = len(set(i for i in training_sample['click_history']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions=training_sample[['user_embedding','click_history']].reset_index(drop=True)\n",
    "interactions['click'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interactions['user_embedding'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# all_items=set(embedding_dict_1.keys())\n",
    "# additional_item_ids = all_items - set(interactions.loc[interactions['click'] == 1, 'click_history'])\n",
    "# additional_rows = []\n",
    "\n",
    "# # For each unique user ID, randomly select k item IDs from additional item IDs\n",
    "# k = 10  # Example value of k\n",
    "# for user_id in interactions['user_embedding'].unique():\n",
    "#     additional_item_ids_for_user = random.sample(list(additional_item_ids), k)\n",
    "#     for additional_item_id in additional_item_ids_for_user:\n",
    "#         additional_rows.append({'user_embedding': user_id, 'click_history': additional_item_id, 'click': 0})\n",
    "\n",
    "# # Concatenate original DataFrame and additional DataFrame\n",
    "# additional_df = pd.DataFrame(additional_rows)\n",
    "# interactions1 = pd.concat([interactions, additional_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions2 = interactions1.sample(frac=1).reset_index(drop=True)\n",
    "# interactions2\n",
    "# user_item_interactions = list(interactions.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_interactions = {}\n",
    "# for user, item in user_item_interactions:\n",
    "#     if user not in positive_interactions:\n",
    "#         positive_interactions[user] = []\n",
    "#     positive_interactions[user].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items=set(embedding_dict_1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_pairs = []\n",
    "# for user, pos_items in positive_interactions.items():\n",
    "#     for pos_item in pos_items:\n",
    "#         # Positive interaction\n",
    "#         # user_item_pairs.append((user, pos_item))\n",
    "        \n",
    "#         # Negative interactions (randomly sampled)\n",
    "#         neg_items = all_items - set(pos_items)  # All items not interacted with\n",
    "#         neg_item = np.random.choice(list(neg_items))  # Randomly select a negative item\n",
    "#         user_item_pairs.append((user, pos_item, neg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_pairs_list = []\n",
    "# embedding_size=50\n",
    "# default_embedding = np.zeros(embedding_size)\n",
    "# for triple in user_item_pairs:\n",
    "#     triple_with_values = tuple(embedding_dict_1.get(item_id,default_embedding) for item_id in triple)\n",
    "#     user_item_pairs_list.append(triple_with_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_item_pairs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# # Define the Neural Collaborative Filtering (NCF) model\n",
    "# class NCF(nn.Module):\n",
    "#     def __init__(self, num_users, num_items, embedding_dim=64, hidden_dim=64):\n",
    "#         super(NCF, self).__init__()\n",
    "#         self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "#         self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "#         self.fc_layers = nn.Sequential(\n",
    "#             nn.Linear(2 * embedding_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, user_ids, item_ids,train=True):\n",
    "#         # print(user_ids)\n",
    "#         user_embeds = user_ids #torch.tensor([embedding_dict_1[item_id] for item_id in user_ids])\n",
    "#         item_embeds = item_ids#torch.tensor([embedding_dict_1[item_id] for item_id in item_ids])  # Retrieve item embeddings\n",
    "#         if train:\n",
    "#             concat_embeds = torch.cat([user_embeds, item_embeds], dim=1)\n",
    "#         else:\n",
    "#             concat_embeds = torch.cat([user_embeds, item_embeds], dim=0)\n",
    "#         output = self.fc_layers(concat_embeds)\n",
    "#         return output.squeeze()\n",
    "\n",
    "# # Preprocess data\n",
    "# # user_encoder = LabelEncoder()\n",
    "# # item_encoder = LabelEncoder()\n",
    "# # training_sample['user_embedding'] = user_encoder.fit_transform(training_sample['user_embedding'])\n",
    "# # training_sample['item_id'] = item_encoder.fit_transform(training_sample['click_history'])\n",
    "# num_users = len(set(interactions['user_embedding']))\n",
    "# num_items = len(set(interactions['click_history']))\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# # train_df, test_df = train_test_split(interactions2, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# class DataFrameDataset(Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         self.data = dataframe\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.data.iloc[idx]\n",
    "#         # print(row['user_embedding'])\n",
    "#         default_embedding = torch.zeros(50, dtype=torch.float32)\n",
    "#         user_id = torch.tensor(embedding_dict_1.get(row['user_embedding'],default_embedding))\n",
    "#         item_id = torch.tensor(embedding_dict_1.get(row['click_history'],default_embedding))\n",
    "#         rating = torch.tensor(row['click']).to(torch.float32)\n",
    "#         return {'user_id': user_id, 'item_id': item_id, 'rating': rating}\n",
    "    \n",
    "# dataset=DataFrameDataset(interactions2)\n",
    "# train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Instantiate the NCF model\n",
    "# model = NCF(num_users=num_users, num_items=num_items, embedding_dim=50, hidden_dim=50)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for binary classification\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # Training loop\n",
    "# # for batch in train_loader:\n",
    "# #     print(batch)\n",
    "# num_epochs = 1\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for batch in train_loader:\n",
    "      \n",
    "#         user_ids = batch['user_id']\n",
    "        \n",
    "#         item_ids = batch['item_id']\n",
    "#         ratings = batch['rating']\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(user_ids, item_ids)\n",
    "#         # print(outputs.dtype)\n",
    "#         # print(ratings.dtype)\n",
    "#         loss = criterion(outputs, ratings)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"epoch: {epoch}, loss: {loss}\")\n",
    "\n",
    "# # Example of making predictions\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "    \n",
    "#     user_embedding = torch.tensor(embedding_dict_1.get(interactions.loc[656,'user_embedding'])) # Example user ID\n",
    "#     item_id =torch.tensor(embedding_dict_1.get(interactions.loc[659,'click_history']))  # Example item ID\n",
    "#     rating_prediction = torch.sigmoid(model(user_embedding, item_id,train=False))\n",
    "#     print(rating_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_ones = (interactions2['click'] == 1).sum()\n",
    "\n",
    "# print(\"Number of 1's in the 'click' column:\", num_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feather_path=base_path/ Path(\"MINDlarge_train/choice_model_data.feather\")\n",
    "# interactions2.to_feather(feather_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
